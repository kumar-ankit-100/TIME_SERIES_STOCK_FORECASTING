{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44edf496-242e-4725-9310-b928b33c885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed: ADANIPORTS.csv\n",
      "✅ Processed: ASIANPAINT.csv\n",
      "✅ Processed: AXISBANK.csv\n",
      "✅ Processed: BAJAJ-AUTO.csv\n",
      "✅ Processed: BAJAJFINSV.csv\n",
      "✅ Processed: BAJFINANCE.csv\n",
      "✅ Processed: BHARTIARTL.csv\n",
      "✅ Processed: BPCL.csv\n",
      "✅ Processed: BRITANNIA.csv\n",
      "✅ Processed: CIPLA.csv\n",
      "✅ Processed: COALINDIA.csv\n",
      "✅ Processed: DRREDDY.csv\n",
      "✅ Processed: EICHERMOT.csv\n",
      "✅ Processed: GAIL.csv\n",
      "✅ Processed: GRASIM.csv\n",
      "✅ Processed: HCLTECH.csv\n",
      "✅ Processed: HDFC.csv\n",
      "✅ Processed: HDFCBANK.csv\n",
      "✅ Processed: HEROMOTOCO.csv\n",
      "✅ Processed: HINDALCO.csv\n",
      "✅ Processed: HINDUNILVR.csv\n",
      "✅ Processed: ICICIBANK.csv\n",
      "✅ Processed: INDUSINDBK.csv\n",
      "✅ Processed: INFY.csv\n",
      "✅ Processed: IOC.csv\n",
      "✅ Processed: ITC.csv\n",
      "✅ Processed: JSWSTEEL.csv\n",
      "✅ Processed: KOTAKBANK.csv\n",
      "✅ Processed: LT.csv\n",
      "✅ Processed: MARUTI.csv\n",
      "✅ Processed: MM.csv\n",
      "✅ Processed: NESTLEIND.csv\n",
      "✅ Processed: NIFTY50_all.csv\n",
      "✅ Processed: NTPC.csv\n",
      "✅ Processed: ONGC.csv\n",
      "✅ Processed: POWERGRID.csv\n",
      "✅ Processed: RELIANCE.csv\n",
      "✅ Processed: SBIN.csv\n",
      "✅ Processed: SHREECEM.csv\n",
      "✅ Processed: SUNPHARMA.csv\n",
      "✅ Processed: TATAMOTORS.csv\n",
      "✅ Processed: TATASTEEL.csv\n",
      "✅ Processed: TCS.csv\n",
      "✅ Processed: TECHM.csv\n",
      "✅ Processed: TITAN.csv\n",
      "✅ Processed: ULTRACEMCO.csv\n",
      "✅ Processed: UPL.csv\n",
      "✅ Processed: VEDL.csv\n",
      "✅ Processed: WIPRO.csv\n",
      "✅ Processed: ZEEL.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Path to folder containing raw CSV files\n",
    "data_folder = r'C:\\Users\\jahna\\OneDrive\\Documents\\TIME_SERIES_STOCK_FORECASTING\\Data\\nifty50-stock-market-data'\n",
    "\n",
    "# Output folder for processed files\n",
    "output_folder =  r'C:\\Users\\jahna\\OneDrive\\Documents\\TIME_SERIES_STOCK_FORECASTING\\PREPROCESSING\\CLEAN_DATA_FINAL'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Utility Functions\n",
    "# ------------------------------\n",
    "\n",
    "def clean_column_names(df):\n",
    "    \"\"\"Standardizes column names to lowercase with snake_case.\"\"\"\n",
    "    df.columns = [col.strip().lower().replace(\" \", \"_\").replace(\"%\", \"percent\").replace(\"-\", \"_\") for col in df.columns]\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, columns, z_thresh=4):\n",
    "    \"\"\"Removes outliers from numeric columns based on Z-score.\"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df = df[(np.abs(zscore(df[col].fillna(0))) < z_thresh)]\n",
    "    return df\n",
    "\n",
    "# Main Preprocessing Function\n",
    "\n",
    "\n",
    "def preprocess_stock_data(df):\n",
    "    # 1. Standardize column names\n",
    "    df = clean_column_names(df)\n",
    "\n",
    "    # 2. Convert 'date' column to datetime\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df.dropna(subset=['date'], inplace=True)\n",
    "    df.sort_values('date', inplace=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    # 3. Drop duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 4. Drop columns with >80% missing values\n",
    "    threshold = 0.8 * len(df)\n",
    "    df.dropna(thresh=threshold, axis=1, inplace=True)\n",
    "\n",
    "    # 5. Convert applicable columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col not in ['symbol', 'series']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 6. Impute missing numeric values with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "    # 7. Sanity checks: remove rows with invalid or zero values\n",
    "    if 'volume' in df.columns:\n",
    "        df = df[df['volume'] > 0]\n",
    "    if 'turnover' in df.columns:\n",
    "        df = df[df['turnover'] > 0]\n",
    "\n",
    "    # 8. Remove outliers using Z-score\n",
    "    df = remove_outliers(df, ['close', 'volume', 'turnover'])\n",
    "\n",
    "    # 9. Add date/time-based features\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    df['weekday'] = df.index.dayofweek\n",
    "    df['is_month_start'] = df.index.is_month_start\n",
    "    df['is_month_end'] = df.index.is_month_end\n",
    "\n",
    "    # 10. Rolling statistics\n",
    "    df['rolling_5_close'] = df['close'].rolling(5).mean()\n",
    "    df['rolling_20_close'] = df['close'].rolling(20).mean()\n",
    "    df['rolling_5_volume'] = df['volume'].rolling(5).mean()\n",
    "    df['rolling_std_10'] = df['close'].rolling(10).std()\n",
    "\n",
    "    # 11. Lag features\n",
    "    df['lag_1_close'] = df['close'].shift(1)\n",
    "    df['lag_3_close'] = df['close'].shift(3)\n",
    "\n",
    "    # Reset index to make 'date' a column again\n",
    "    return df.reset_index()\n",
    "\n",
    "# ------------------------------\n",
    "# Loop Over All CSV Files\n",
    "# ------------------------------\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        input_path = os.path.join(data_folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "\n",
    "            # --- Step 1: Check if a date column exists ---\n",
    "            # Case insensitive check\n",
    "            date_cols = [col for col in df.columns if col.lower() == 'date']\n",
    "            if not date_cols:\n",
    "                raise ValueError(\"Missing 'Date' column.\")\n",
    "\n",
    "            # Rename to standard 'date'\n",
    "            df.rename(columns={date_cols[0]: 'date'}, inplace=True)\n",
    "\n",
    "            # --- Step 2: Try converting to datetime and check for valid entries ---\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "            if df['date'].isna().all():\n",
    "                raise ValueError(\"'Date' column has no valid datetime entries.\")\n",
    "\n",
    "            # Proceed with full preprocessing\n",
    "            cleaned_df = preprocess_stock_data(df)\n",
    "            output_path = os.path.join(output_folder, file)\n",
    "            cleaned_df.to_csv(output_path, index=False)\n",
    "            print(f\"✅ Processed: {file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipped: {file} | Reason: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a512db-1600-45b9-a829-5f043184314a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
